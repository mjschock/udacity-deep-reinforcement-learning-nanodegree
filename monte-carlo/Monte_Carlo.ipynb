{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Methods\n",
    "\n",
    "In this notebook, you will write your own implementations of many Monte Carlo (MC) algorithms. \n",
    "\n",
    "While we have provided some starter code, you are welcome to erase these hints and write your code from scratch.\n",
    "\n",
    "### Part 0: Explore BlackjackEnv\n",
    "\n",
    "We begin by importing the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from plot_utils import plot_blackjack_values, plot_policy\n",
    "\n",
    "from datagrep.agents import Agent\n",
    "from datagrep.environments import Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code cell below to create an instance of the [Blackjack](https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py) environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = Environment('Blackjack-v0')\n",
    "env.seed(RANDOM_SEED)\n",
    "agent = Agent()\n",
    "agent.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each state is a 3-tuple of:\n",
    "- the player's current sum $\\in \\{0, 1, \\ldots, 31\\}$,\n",
    "- the dealer's face up card $\\in \\{1, \\ldots, 10\\}$, and\n",
    "- whether or not the player has a usable ace (`no` $=0$, `yes` $=1$).\n",
    "\n",
    "The agent has two potential actions:\n",
    "\n",
    "```\n",
    "    STICK = 0\n",
    "    HIT = 1\n",
    "```\n",
    "Verify this by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuple(Discrete(32), Discrete(11), Discrete(2))\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "# agent.sense(env)\n",
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the code cell below to play Blackjack with a random policy.  \n",
    "\n",
    "(_The code currently plays Blackjack three times - feel free to change this number, or to run the cell multiple times.  The cell is designed for you to get some experience with the output that is returned as the agent interacts with the environment._)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End game! Reward:  -1\n",
      "You lost :(\n",
      "\n",
      "End game! Reward:  -1\n",
      "You lost :(\n",
      "\n",
      "End game! Reward:  -1.0\n",
      "You lost :(\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(3):\n",
    "    env.reset()\n",
    "\n",
    "    while True:\n",
    "        observation, available_actions, available_action_probabilities, reward, done = agent.sense(env)\n",
    "        action = agent.act(env, observation, available_actions, available_action_probabilities)\n",
    "\n",
    "        if done:\n",
    "            print('End game! Reward: ', reward)\n",
    "            print('You won :)\\n') if reward > 0 else print('You lost :(\\n')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: MC Prediction\n",
    "\n",
    "In this section, you will write your own implementation of MC prediction (for estimating the action-value function).  \n",
    "\n",
    "We will begin by investigating a policy where the player _almost_ always sticks if the sum of her cards exceeds 18.  In particular, she selects action `STICK` with 80% probability if the sum is greater than 18; and, if the sum is 18 or below, she selects action `HIT` with 80% probability.  The function `generate_episode_from_limit_stochastic` samples an episode using this policy. \n",
    "\n",
    "The function accepts as **input**:\n",
    "- `bj_env`: This is an instance of OpenAI Gym's Blackjack environment.\n",
    "\n",
    "It returns as **output**:\n",
    "- `episode`: This is a list of (state, action, reward) tuples (of tuples) and corresponds to $(S_0, A_0, R_1, \\ldots, S_{T-1}, A_{T-1}, R_{T})$, where $T$ is the final time step.  In particular, `episode[i]` returns $(S_i, A_i, R_{i+1})$, and `episode[i][0]`, `episode[i][1]`, and `episode[i][2]` return $S_i$, $A_i$, and $R_{i+1}$, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observation_wrapper(observation, available_actions, available_action_probabilities):\n",
    "    available_action_probabilities = (\n",
    "        [0.8, 0.2] if observation[0] > 18 else [0.2, 0.8]\n",
    "    )\n",
    "    return observation, available_actions, available_action_probabilities\n",
    "\n",
    "def generate_episode_from_limit_stochastic(mc_agent, bj_env):\n",
    "    episode = []\n",
    "    bj_env.reset()\n",
    "    (\n",
    "        observation,\n",
    "        available_actions,\n",
    "        available_action_probabilities,\n",
    "        value,\n",
    "        done\n",
    "    ) = mc_agent.sense(bj_env, observation_wrapper=observation_wrapper)\n",
    "\n",
    "    while True:\n",
    "        action = mc_agent.act(\n",
    "            bj_env, observation, available_actions, available_action_probabilities\n",
    "        )\n",
    "        \n",
    "        last_observation = observation\n",
    "\n",
    "        (\n",
    "            observation,\n",
    "            available_actions,\n",
    "            available_action_probabilities,\n",
    "            reward,\n",
    "            done\n",
    "        ) = mc_agent.sense(bj_env, observation_wrapper=observation_wrapper)\n",
    "\n",
    "        episode.append((last_observation, action, reward))\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return episode\n",
    "\n",
    "class Predictor():\n",
    "    def learn(self, trajectories, gamma, alpha):\n",
    "        pass\n",
    "\n",
    "    def predict(self, observation, available_actions, available_action_probabilities):\n",
    "        pass\n",
    "\n",
    "# class QualityPredictor(Predictor):\n",
    "#     def learn(self, trajectories, gamma, alpha):\n",
    "#         for trajectory in trajectories:\n",
    "#             for i, (observation, action, reward) in enumerate(trajectory):\n",
    "#                 sum_of_discounted_rewards = sum(\n",
    "#                     [\n",
    "#                         gamma ** (j - i) * trajectory[j][-1]\n",
    "#                         for j in range(i, len(trajectory))\n",
    "#                     ]\n",
    "#                 )\n",
    "\n",
    "#                 self.observation_action_visit_count[observation][action] += 1\n",
    "#                 self.observation_action_returns_sum[observation][\n",
    "#                     action\n",
    "#                 ] += sum_of_discounted_rewards\n",
    "#                 self.observation_action_value[observation][action] = (\n",
    "#                     self.observation_action_returns_sum[observation][action]\n",
    "#                     / self.observation_action_visit_count[observation][action]\n",
    "#                 )\n",
    "        \n",
    "# class ActionPredictor(Predictor):\n",
    "#     def predict(self, observation, available_actions, available_action_probabilities):\n",
    "#         return max(  # TODO: randomly choose among duplicates\n",
    "#             range(len(available_actions)),\n",
    "#             key=lambda index: self.observation_action_value[observation][\n",
    "#                 available_actions[index]\n",
    "#             ],\n",
    "#         )\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "class MonteCarloPredictor(Predictor):\n",
    "    model = None\n",
    "    observation_action_visit_count = defaultdict(lambda: Counter())\n",
    "    observation_action_returns_sum = defaultdict(lambda: defaultdict(float))\n",
    "    observation_action_value = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "    # predict is always about selecting the next action, even if internally there's a model/quality\n",
    "    # instead, have a Controller that selects an action i.e. controller.select_next_action()\n",
    "    # evaluator.evaluate_current_state()\n",
    "    # modeler.predict_transition() -> predict the physics of the environment\n",
    "    def predict(self, observation, available_actions, available_action_probabilities):\n",
    "        return max(  # TODO: randomly choose among duplicates\n",
    "            range(len(available_actions)),\n",
    "            key=lambda index: self.observation_action_value[observation][\n",
    "                available_actions[index]\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    def predict_action_value(self, observation, action):\n",
    "        # return self.model.predict(observation, action)\n",
    "        return self.observation_action_value[observation][action]\n",
    "\n",
    "    def learn(self, trajectories, gamma, alpha):\n",
    "        for trajectory in trajectories:\n",
    "            for i, (observation, action, reward) in enumerate(trajectory):\n",
    "                sum_of_discounted_rewards = sum(\n",
    "                    [\n",
    "                        gamma ** (j - i) * trajectory[j][-1]\n",
    "                        for j in range(i, len(trajectory))\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                self.observation_action_visit_count[observation][action] += 1\n",
    "                self.observation_action_returns_sum[observation][\n",
    "                    action\n",
    "                ] += sum_of_discounted_rewards\n",
    "                self.observation_action_value[observation][action] = (\n",
    "                    self.observation_action_returns_sum[observation][action]\n",
    "                    / self.observation_action_visit_count[observation][action]\n",
    "                )\n",
    "\n",
    "predictor = MonteCarloPredictor()\n",
    "    \n",
    "agent = Agent(\n",
    "#     model_predictor=None # predict the physics of the universe/environment \n",
    "    action_predictor=predictor, # ActionPredictor; predict the next action\n",
    "    quality_predictor=predictor # predict the value of the current state\n",
    "#     predictor=MonteCarloPredictor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the code cell below to play Blackjack with the policy. \n",
    "\n",
    "(*The code currently plays Blackjack three times - feel free to change this number, or to run the cell multiple times.  The cell is designed for you to gain some familiarity with the output of the `generate_episode_from_limit_stochastic` function.*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((18, 10, False), ActResult(action=0), -1.0)]\n",
      "[((21, 1, True), ActResult(action=0), 0.0)]\n",
      "[((20, 2, False), ActResult(action=0), 0.0)]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(generate_episode_from_limit_stochastic(agent, env))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you are ready to write your own implementation of MC prediction.  Feel free to implement either first-visit or every-visit MC prediction; in the case of the Blackjack environment, the techniques are equivalent.\n",
    "\n",
    "Your algorithm has three arguments:\n",
    "- `env`: This is an instance of an OpenAI Gym environment.\n",
    "- `num_episodes`: This is the number of episodes that are generated through agent-environment interaction.\n",
    "- `generate_episode`: This is a function that returns an episode of interaction.\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "\n",
    "The algorithm returns as output:\n",
    "- `Q`: This is a dictionary (of one-dimensional arrays) where `Q[s][a]` is the estimated action value corresponding to state `s` and action `a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_prediction_q(agent, env, num_episodes, generate_episode, gamma=1.0):\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        # monitor progress\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        episode = generate_episode(agent, env)\n",
    "        agent.learn([episode], gamma)\n",
    "\n",
    "    return agent.quality_predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the cell below to obtain the action-value function estimate $Q$.  We have also plotted the corresponding state-value function.\n",
    "\n",
    "To check the accuracy of your implementation, compare the plot below to the corresponding plot in the solutions notebook **Monte_Carlo_Solution.ipynb**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 500000/500000."
     ]
    }
   ],
   "source": [
    "# obtain the action-value function\n",
    "quality_predictor = mc_prediction_q(agent, env, 500000, generate_episode_from_limit_stochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = {}\n",
    "\n",
    "for current_sum in list(range(32)):\n",
    "    for dealers_card in list(range(11)):\n",
    "        for usable_ace in (True, False):\n",
    "            vals = []\n",
    "\n",
    "            for action in (0, 1):\n",
    "                obs = (current_sum, dealers_card, usable_ace)\n",
    "                vals.append(quality_predictor.predict_action_value(obs, action))\n",
    "\n",
    "            Q[obs] = vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q[(4, 1, False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0, True): [0.0, 0.0],\n",
       " (0, 0, False): [0.0, 0.0],\n",
       " (0, 1, True): [0.0, 0.0],\n",
       " (0, 1, False): [0.0, 0.0],\n",
       " (0, 2, True): [0.0, 0.0],\n",
       " (0, 2, False): [0.0, 0.0],\n",
       " (0, 3, True): [0.0, 0.0],\n",
       " (0, 3, False): [0.0, 0.0],\n",
       " (0, 4, True): [0.0, 0.0],\n",
       " (0, 4, False): [0.0, 0.0],\n",
       " (0, 5, True): [0.0, 0.0],\n",
       " (0, 5, False): [0.0, 0.0],\n",
       " (0, 6, True): [0.0, 0.0],\n",
       " (0, 6, False): [0.0, 0.0],\n",
       " (0, 7, True): [0.0, 0.0],\n",
       " (0, 7, False): [0.0, 0.0],\n",
       " (0, 8, True): [0.0, 0.0],\n",
       " (0, 8, False): [0.0, 0.0],\n",
       " (0, 9, True): [0.0, 0.0],\n",
       " (0, 9, False): [0.0, 0.0],\n",
       " (0, 10, True): [0.0, 0.0],\n",
       " (0, 10, False): [0.0, 0.0],\n",
       " (1, 0, True): [0.0, 0.0],\n",
       " (1, 0, False): [0.0, 0.0],\n",
       " (1, 1, True): [0.0, 0.0],\n",
       " (1, 1, False): [0.0, 0.0],\n",
       " (1, 2, True): [0.0, 0.0],\n",
       " (1, 2, False): [0.0, 0.0],\n",
       " (1, 3, True): [0.0, 0.0],\n",
       " (1, 3, False): [0.0, 0.0],\n",
       " (1, 4, True): [0.0, 0.0],\n",
       " (1, 4, False): [0.0, 0.0],\n",
       " (1, 5, True): [0.0, 0.0],\n",
       " (1, 5, False): [0.0, 0.0],\n",
       " (1, 6, True): [0.0, 0.0],\n",
       " (1, 6, False): [0.0, 0.0],\n",
       " (1, 7, True): [0.0, 0.0],\n",
       " (1, 7, False): [0.0, 0.0],\n",
       " (1, 8, True): [0.0, 0.0],\n",
       " (1, 8, False): [0.0, 0.0],\n",
       " (1, 9, True): [0.0, 0.0],\n",
       " (1, 9, False): [0.0, 0.0],\n",
       " (1, 10, True): [0.0, 0.0],\n",
       " (1, 10, False): [0.0, 0.0],\n",
       " (2, 0, True): [0.0, 0.0],\n",
       " (2, 0, False): [0.0, 0.0],\n",
       " (2, 1, True): [0.0, 0.0],\n",
       " (2, 1, False): [0.0, 0.0],\n",
       " (2, 2, True): [0.0, 0.0],\n",
       " (2, 2, False): [0.0, 0.0],\n",
       " (2, 3, True): [0.0, 0.0],\n",
       " (2, 3, False): [0.0, 0.0],\n",
       " (2, 4, True): [0.0, 0.0],\n",
       " (2, 4, False): [0.0, 0.0],\n",
       " (2, 5, True): [0.0, 0.0],\n",
       " (2, 5, False): [0.0, 0.0],\n",
       " (2, 6, True): [0.0, 0.0],\n",
       " (2, 6, False): [0.0, 0.0],\n",
       " (2, 7, True): [0.0, 0.0],\n",
       " (2, 7, False): [0.0, 0.0],\n",
       " (2, 8, True): [0.0, 0.0],\n",
       " (2, 8, False): [0.0, 0.0],\n",
       " (2, 9, True): [0.0, 0.0],\n",
       " (2, 9, False): [0.0, 0.0],\n",
       " (2, 10, True): [0.0, 0.0],\n",
       " (2, 10, False): [0.0, 0.0],\n",
       " (3, 0, True): [0.0, 0.0],\n",
       " (3, 0, False): [0.0, 0.0],\n",
       " (3, 1, True): [0.0, 0.0],\n",
       " (3, 1, False): [0.0, 0.0],\n",
       " (3, 2, True): [0.0, 0.0],\n",
       " (3, 2, False): [0.0, 0.0],\n",
       " (3, 3, True): [0.0, 0.0],\n",
       " (3, 3, False): [0.0, 0.0],\n",
       " (3, 4, True): [0.0, 0.0],\n",
       " (3, 4, False): [0.0, 0.0],\n",
       " (3, 5, True): [0.0, 0.0],\n",
       " (3, 5, False): [0.0, 0.0],\n",
       " (3, 6, True): [0.0, 0.0],\n",
       " (3, 6, False): [0.0, 0.0],\n",
       " (3, 7, True): [0.0, 0.0],\n",
       " (3, 7, False): [0.0, 0.0],\n",
       " (3, 8, True): [0.0, 0.0],\n",
       " (3, 8, False): [0.0, 0.0],\n",
       " (3, 9, True): [0.0, 0.0],\n",
       " (3, 9, False): [0.0, 0.0],\n",
       " (3, 10, True): [0.0, 0.0],\n",
       " (3, 10, False): [0.0, 0.0],\n",
       " (4, 0, True): [0.0, 0.0],\n",
       " (4, 0, False): [0.0, 0.0],\n",
       " (4, 1, True): [0.0, 0.0],\n",
       " (4, 1, False): [0.0, 0.0],\n",
       " (4, 2, True): [0.0, 0.0],\n",
       " (4, 2, False): [0.0, 0.0],\n",
       " (4, 3, True): [0.0, 0.0],\n",
       " (4, 3, False): [0.0, 0.0],\n",
       " (4, 4, True): [0.0, 0.0],\n",
       " (4, 4, False): [0.0, 0.0],\n",
       " (4, 5, True): [0.0, 0.0],\n",
       " (4, 5, False): [0.0, 0.0],\n",
       " (4, 6, True): [0.0, 0.0],\n",
       " (4, 6, False): [0.0, 0.0],\n",
       " (4, 7, True): [0.0, 0.0],\n",
       " (4, 7, False): [0.0, 0.0],\n",
       " (4, 8, True): [0.0, 0.0],\n",
       " (4, 8, False): [0.0, 0.0],\n",
       " (4, 9, True): [0.0, 0.0],\n",
       " (4, 9, False): [0.0, 0.0],\n",
       " (4, 10, True): [0.0, 0.0],\n",
       " (4, 10, False): [0.0, 0.0],\n",
       " (5, 0, True): [0.0, 0.0],\n",
       " (5, 0, False): [0.0, 0.0],\n",
       " (5, 1, True): [0.0, 0.0],\n",
       " (5, 1, False): [0.0, 0.0],\n",
       " (5, 2, True): [0.0, 0.0],\n",
       " (5, 2, False): [0.0, 0.0],\n",
       " (5, 3, True): [0.0, 0.0],\n",
       " (5, 3, False): [0.0, 0.0],\n",
       " (5, 4, True): [0.0, 0.0],\n",
       " (5, 4, False): [0.0, 0.0],\n",
       " (5, 5, True): [0.0, 0.0],\n",
       " (5, 5, False): [0.0, 0.0],\n",
       " (5, 6, True): [0.0, 0.0],\n",
       " (5, 6, False): [0.0, 0.0],\n",
       " (5, 7, True): [0.0, 0.0],\n",
       " (5, 7, False): [0.0, 0.0],\n",
       " (5, 8, True): [0.0, 0.0],\n",
       " (5, 8, False): [0.0, 0.0],\n",
       " (5, 9, True): [0.0, 0.0],\n",
       " (5, 9, False): [0.0, 0.0],\n",
       " (5, 10, True): [0.0, 0.0],\n",
       " (5, 10, False): [0.0, 0.0],\n",
       " (6, 0, True): [0.0, 0.0],\n",
       " (6, 0, False): [0.0, 0.0],\n",
       " (6, 1, True): [0.0, 0.0],\n",
       " (6, 1, False): [0.0, 0.0],\n",
       " (6, 2, True): [0.0, 0.0],\n",
       " (6, 2, False): [0.0, 0.0],\n",
       " (6, 3, True): [0.0, 0.0],\n",
       " (6, 3, False): [0.0, 0.0],\n",
       " (6, 4, True): [0.0, 0.0],\n",
       " (6, 4, False): [0.0, 0.0],\n",
       " (6, 5, True): [0.0, 0.0],\n",
       " (6, 5, False): [0.0, 0.0],\n",
       " (6, 6, True): [0.0, 0.0],\n",
       " (6, 6, False): [0.0, 0.0],\n",
       " (6, 7, True): [0.0, 0.0],\n",
       " (6, 7, False): [0.0, 0.0],\n",
       " (6, 8, True): [0.0, 0.0],\n",
       " (6, 8, False): [0.0, 0.0],\n",
       " (6, 9, True): [0.0, 0.0],\n",
       " (6, 9, False): [0.0, 0.0],\n",
       " (6, 10, True): [0.0, 0.0],\n",
       " (6, 10, False): [0.0, 0.0],\n",
       " (7, 0, True): [0.0, 0.0],\n",
       " (7, 0, False): [0.0, 0.0],\n",
       " (7, 1, True): [0.0, 0.0],\n",
       " (7, 1, False): [0.0, 0.0],\n",
       " (7, 2, True): [0.0, 0.0],\n",
       " (7, 2, False): [0.0, 0.0],\n",
       " (7, 3, True): [0.0, 0.0],\n",
       " (7, 3, False): [0.0, 0.0],\n",
       " (7, 4, True): [0.0, 0.0],\n",
       " (7, 4, False): [0.0, 0.0],\n",
       " (7, 5, True): [0.0, 0.0],\n",
       " (7, 5, False): [0.0, 0.0],\n",
       " (7, 6, True): [0.0, 0.0],\n",
       " (7, 6, False): [0.0, 0.0],\n",
       " (7, 7, True): [0.0, 0.0],\n",
       " (7, 7, False): [0.0, 0.0],\n",
       " (7, 8, True): [0.0, 0.0],\n",
       " (7, 8, False): [0.0, 0.0],\n",
       " (7, 9, True): [0.0, 0.0],\n",
       " (7, 9, False): [0.0, 0.0],\n",
       " (7, 10, True): [0.0, 0.0],\n",
       " (7, 10, False): [0.0, 0.0],\n",
       " (8, 0, True): [0.0, 0.0],\n",
       " (8, 0, False): [0.0, 0.0],\n",
       " (8, 1, True): [0.0, 0.0],\n",
       " (8, 1, False): [0.0, 0.0],\n",
       " (8, 2, True): [0.0, 0.0],\n",
       " (8, 2, False): [0.0, 0.0],\n",
       " (8, 3, True): [0.0, 0.0],\n",
       " (8, 3, False): [0.0, 0.0],\n",
       " (8, 4, True): [0.0, 0.0],\n",
       " (8, 4, False): [0.0, 0.0],\n",
       " (8, 5, True): [0.0, 0.0],\n",
       " (8, 5, False): [0.0, 0.0],\n",
       " (8, 6, True): [0.0, 0.0],\n",
       " (8, 6, False): [0.0, 0.0],\n",
       " (8, 7, True): [0.0, 0.0],\n",
       " (8, 7, False): [0.0, 0.0],\n",
       " (8, 8, True): [0.0, 0.0],\n",
       " (8, 8, False): [0.0, 0.0],\n",
       " (8, 9, True): [0.0, 0.0],\n",
       " (8, 9, False): [0.0, 0.0],\n",
       " (8, 10, True): [0.0, 0.0],\n",
       " (8, 10, False): [0.0, 0.0],\n",
       " (9, 0, True): [0.0, 0.0],\n",
       " (9, 0, False): [0.0, 0.0],\n",
       " (9, 1, True): [0.0, 0.0],\n",
       " (9, 1, False): [0.0, 0.0],\n",
       " (9, 2, True): [0.0, 0.0],\n",
       " (9, 2, False): [0.0, 0.0],\n",
       " (9, 3, True): [0.0, 0.0],\n",
       " (9, 3, False): [0.0, 0.0],\n",
       " (9, 4, True): [0.0, 0.0],\n",
       " (9, 4, False): [0.0, 0.0],\n",
       " (9, 5, True): [0.0, 0.0],\n",
       " (9, 5, False): [0.0, 0.0],\n",
       " (9, 6, True): [0.0, 0.0],\n",
       " (9, 6, False): [0.0, 0.0],\n",
       " (9, 7, True): [0.0, 0.0],\n",
       " (9, 7, False): [0.0, 0.0],\n",
       " (9, 8, True): [0.0, 0.0],\n",
       " (9, 8, False): [0.0, 0.0],\n",
       " (9, 9, True): [0.0, 0.0],\n",
       " (9, 9, False): [0.0, 0.0],\n",
       " (9, 10, True): [0.0, 0.0],\n",
       " (9, 10, False): [0.0, 0.0],\n",
       " (10, 0, True): [0.0, 0.0],\n",
       " (10, 0, False): [0.0, 0.0],\n",
       " (10, 1, True): [0.0, 0.0],\n",
       " (10, 1, False): [0.0, 0.0],\n",
       " (10, 2, True): [0.0, 0.0],\n",
       " (10, 2, False): [0.0, 0.0],\n",
       " (10, 3, True): [0.0, 0.0],\n",
       " (10, 3, False): [0.0, 0.0],\n",
       " (10, 4, True): [0.0, 0.0],\n",
       " (10, 4, False): [0.0, 0.0],\n",
       " (10, 5, True): [0.0, 0.0],\n",
       " (10, 5, False): [0.0, 0.0],\n",
       " (10, 6, True): [0.0, 0.0],\n",
       " (10, 6, False): [0.0, 0.0],\n",
       " (10, 7, True): [0.0, 0.0],\n",
       " (10, 7, False): [0.0, 0.0],\n",
       " (10, 8, True): [0.0, 0.0],\n",
       " (10, 8, False): [0.0, 0.0],\n",
       " (10, 9, True): [0.0, 0.0],\n",
       " (10, 9, False): [0.0, 0.0],\n",
       " (10, 10, True): [0.0, 0.0],\n",
       " (10, 10, False): [0.0, 0.0],\n",
       " (11, 0, True): [0.0, 0.0],\n",
       " (11, 0, False): [0.0, 0.0],\n",
       " (11, 1, True): [0.0, 0.0],\n",
       " (11, 1, False): [0.0, 0.0],\n",
       " (11, 2, True): [0.0, 0.0],\n",
       " (11, 2, False): [0.0, 0.0],\n",
       " (11, 3, True): [0.0, 0.0],\n",
       " (11, 3, False): [0.0, 0.0],\n",
       " (11, 4, True): [0.0, 0.0],\n",
       " (11, 4, False): [0.0, 0.0],\n",
       " (11, 5, True): [0.0, 0.0],\n",
       " (11, 5, False): [0.0, 0.0],\n",
       " (11, 6, True): [0.0, 0.0],\n",
       " (11, 6, False): [0.0, 0.0],\n",
       " (11, 7, True): [0.0, 0.0],\n",
       " (11, 7, False): [0.0, 0.0],\n",
       " (11, 8, True): [0.0, 0.0],\n",
       " (11, 8, False): [0.0, 0.0],\n",
       " (11, 9, True): [0.0, 0.0],\n",
       " (11, 9, False): [0.0, 0.0],\n",
       " (11, 10, True): [0.0, 0.0],\n",
       " (11, 10, False): [0.0, 0.0],\n",
       " (12, 0, True): [0.0, 0.0],\n",
       " (12, 0, False): [0.0, 0.0],\n",
       " (12, 1, True): [0.0, 0.0],\n",
       " (12, 1, False): [0.0, 0.0],\n",
       " (12, 2, True): [0.0, 0.0],\n",
       " (12, 2, False): [0.0, 0.0],\n",
       " (12, 3, True): [0.0, 0.0],\n",
       " (12, 3, False): [0.0, 0.0],\n",
       " (12, 4, True): [0.0, 0.0],\n",
       " (12, 4, False): [0.0, 0.0],\n",
       " (12, 5, True): [0.0, 0.0],\n",
       " (12, 5, False): [0.0, 0.0],\n",
       " (12, 6, True): [0.0, 0.0],\n",
       " (12, 6, False): [0.0, 0.0],\n",
       " (12, 7, True): [0.0, 0.0],\n",
       " (12, 7, False): [0.0, 0.0],\n",
       " (12, 8, True): [0.0, 0.0],\n",
       " (12, 8, False): [0.0, 0.0],\n",
       " (12, 9, True): [0.0, 0.0],\n",
       " (12, 9, False): [0.0, 0.0],\n",
       " (12, 10, True): [0.0, 0.0],\n",
       " (12, 10, False): [0.0, 0.0],\n",
       " (13, 0, True): [0.0, 0.0],\n",
       " (13, 0, False): [0.0, 0.0],\n",
       " (13, 1, True): [0.0, 0.0],\n",
       " (13, 1, False): [0.0, 0.0],\n",
       " (13, 2, True): [0.0, 0.0],\n",
       " (13, 2, False): [0.0, 0.0],\n",
       " (13, 3, True): [0.0, 0.0],\n",
       " (13, 3, False): [0.0, 0.0],\n",
       " (13, 4, True): [0.0, 0.0],\n",
       " (13, 4, False): [0.0, 0.0],\n",
       " (13, 5, True): [0.0, 0.0],\n",
       " (13, 5, False): [0.0, 0.0],\n",
       " (13, 6, True): [0.0, 0.0],\n",
       " (13, 6, False): [0.0, 0.0],\n",
       " (13, 7, True): [0.0, 0.0],\n",
       " (13, 7, False): [0.0, 0.0],\n",
       " (13, 8, True): [0.0, 0.0],\n",
       " (13, 8, False): [0.0, 0.0],\n",
       " (13, 9, True): [0.0, 0.0],\n",
       " (13, 9, False): [0.0, 0.0],\n",
       " (13, 10, True): [0.0, 0.0],\n",
       " (13, 10, False): [0.0, 0.0],\n",
       " (14, 0, True): [0.0, 0.0],\n",
       " (14, 0, False): [0.0, 0.0],\n",
       " (14, 1, True): [0.0, 0.0],\n",
       " (14, 1, False): [0.0, 0.0],\n",
       " (14, 2, True): [0.0, 0.0],\n",
       " (14, 2, False): [0.0, 0.0],\n",
       " (14, 3, True): [0.0, 0.0],\n",
       " (14, 3, False): [0.0, 0.0],\n",
       " (14, 4, True): [0.0, 0.0],\n",
       " (14, 4, False): [0.0, 0.0],\n",
       " (14, 5, True): [0.0, 0.0],\n",
       " (14, 5, False): [0.0, 0.0],\n",
       " (14, 6, True): [0.0, 0.0],\n",
       " (14, 6, False): [0.0, 0.0],\n",
       " (14, 7, True): [0.0, 0.0],\n",
       " (14, 7, False): [0.0, 0.0],\n",
       " (14, 8, True): [0.0, 0.0],\n",
       " (14, 8, False): [0.0, 0.0],\n",
       " (14, 9, True): [0.0, 0.0],\n",
       " (14, 9, False): [0.0, 0.0],\n",
       " (14, 10, True): [0.0, 0.0],\n",
       " (14, 10, False): [0.0, 0.0],\n",
       " (15, 0, True): [0.0, 0.0],\n",
       " (15, 0, False): [0.0, 0.0],\n",
       " (15, 1, True): [0.0, 0.0],\n",
       " (15, 1, False): [0.0, 0.0],\n",
       " (15, 2, True): [0.0, 0.0],\n",
       " (15, 2, False): [0.0, 0.0],\n",
       " (15, 3, True): [0.0, 0.0],\n",
       " (15, 3, False): [0.0, 0.0],\n",
       " (15, 4, True): [0.0, 0.0],\n",
       " (15, 4, False): [0.0, 0.0],\n",
       " (15, 5, True): [0.0, 0.0],\n",
       " (15, 5, False): [0.0, 0.0],\n",
       " (15, 6, True): [0.0, 0.0],\n",
       " (15, 6, False): [0.0, 0.0],\n",
       " (15, 7, True): [0.0, 0.0],\n",
       " (15, 7, False): [0.0, 0.0],\n",
       " (15, 8, True): [0.0, 0.0],\n",
       " (15, 8, False): [0.0, 0.0],\n",
       " (15, 9, True): [0.0, 0.0],\n",
       " (15, 9, False): [0.0, 0.0],\n",
       " (15, 10, True): [0.0, 0.0],\n",
       " (15, 10, False): [0.0, 0.0],\n",
       " (16, 0, True): [0.0, 0.0],\n",
       " (16, 0, False): [0.0, 0.0],\n",
       " (16, 1, True): [0.0, 0.0],\n",
       " (16, 1, False): [0.0, 0.0],\n",
       " (16, 2, True): [0.0, 0.0],\n",
       " (16, 2, False): [0.0, 0.0],\n",
       " (16, 3, True): [0.0, 0.0],\n",
       " (16, 3, False): [0.0, 0.0],\n",
       " (16, 4, True): [0.0, 0.0],\n",
       " (16, 4, False): [0.0, 0.0],\n",
       " (16, 5, True): [0.0, 0.0],\n",
       " (16, 5, False): [0.0, 0.0],\n",
       " (16, 6, True): [0.0, 0.0],\n",
       " (16, 6, False): [0.0, 0.0],\n",
       " (16, 7, True): [0.0, 0.0],\n",
       " (16, 7, False): [0.0, 0.0],\n",
       " (16, 8, True): [0.0, 0.0],\n",
       " (16, 8, False): [0.0, 0.0],\n",
       " (16, 9, True): [0.0, 0.0],\n",
       " (16, 9, False): [0.0, 0.0],\n",
       " (16, 10, True): [0.0, 0.0],\n",
       " (16, 10, False): [0.0, 0.0],\n",
       " (17, 0, True): [0.0, 0.0],\n",
       " (17, 0, False): [0.0, 0.0],\n",
       " (17, 1, True): [0.0, 0.0],\n",
       " (17, 1, False): [0.0, 0.0],\n",
       " (17, 2, True): [0.0, 0.0],\n",
       " (17, 2, False): [0.0, 0.0],\n",
       " (17, 3, True): [0.0, 0.0],\n",
       " (17, 3, False): [0.0, 0.0],\n",
       " (17, 4, True): [0.0, 0.0],\n",
       " (17, 4, False): [0.0, 0.0],\n",
       " (17, 5, True): [0.0, 0.0],\n",
       " (17, 5, False): [0.0, 0.0],\n",
       " (17, 6, True): [0.0, 0.0],\n",
       " (17, 6, False): [0.0, 0.0],\n",
       " (17, 7, True): [0.0, 0.0],\n",
       " (17, 7, False): [0.0, 0.0],\n",
       " (17, 8, True): [0.0, 0.0],\n",
       " (17, 8, False): [0.0, 0.0],\n",
       " (17, 9, True): [0.0, 0.0],\n",
       " (17, 9, False): [0.0, 0.0],\n",
       " (17, 10, True): [0.0, 0.0],\n",
       " (17, 10, False): [0.0, 0.0],\n",
       " (18, 0, True): [0.0, 0.0],\n",
       " (18, 0, False): [0.0, 0.0],\n",
       " (18, 1, True): [0.0, 0.0],\n",
       " (18, 1, False): [0.0, 0.0],\n",
       " (18, 2, True): [0.0, 0.0],\n",
       " (18, 2, False): [0.0, 0.0],\n",
       " (18, 3, True): [0.0, 0.0],\n",
       " (18, 3, False): [0.0, 0.0],\n",
       " (18, 4, True): [0.0, 0.0],\n",
       " (18, 4, False): [0.0, 0.0],\n",
       " (18, 5, True): [0.0, 0.0],\n",
       " (18, 5, False): [0.0, 0.0],\n",
       " (18, 6, True): [0.0, 0.0],\n",
       " (18, 6, False): [0.0, 0.0],\n",
       " (18, 7, True): [0.0, 0.0],\n",
       " (18, 7, False): [0.0, 0.0],\n",
       " (18, 8, True): [0.0, 0.0],\n",
       " (18, 8, False): [0.0, 0.0],\n",
       " (18, 9, True): [0.0, 0.0],\n",
       " (18, 9, False): [0.0, 0.0],\n",
       " (18, 10, True): [0.0, 0.0],\n",
       " (18, 10, False): [0.0, 0.0],\n",
       " (19, 0, True): [0.0, 0.0],\n",
       " (19, 0, False): [0.0, 0.0],\n",
       " (19, 1, True): [0.0, 0.0],\n",
       " (19, 1, False): [0.0, 0.0],\n",
       " (19, 2, True): [0.0, 0.0],\n",
       " (19, 2, False): [0.0, 0.0],\n",
       " (19, 3, True): [0.0, 0.0],\n",
       " (19, 3, False): [0.0, 0.0],\n",
       " (19, 4, True): [0.0, 0.0],\n",
       " (19, 4, False): [0.0, 0.0],\n",
       " (19, 5, True): [0.0, 0.0],\n",
       " (19, 5, False): [0.0, 0.0],\n",
       " (19, 6, True): [0.0, 0.0],\n",
       " (19, 6, False): [0.0, 0.0],\n",
       " (19, 7, True): [0.0, 0.0],\n",
       " (19, 7, False): [0.0, 0.0],\n",
       " (19, 8, True): [0.0, 0.0],\n",
       " (19, 8, False): [0.0, 0.0],\n",
       " (19, 9, True): [0.0, 0.0],\n",
       " (19, 9, False): [0.0, 0.0],\n",
       " (19, 10, True): [0.0, 0.0],\n",
       " (19, 10, False): [0.0, 0.0],\n",
       " (20, 0, True): [0.0, 0.0],\n",
       " (20, 0, False): [0.0, 0.0],\n",
       " (20, 1, True): [0.0, 0.0],\n",
       " (20, 1, False): [0.0, 0.0],\n",
       " (20, 2, True): [0.0, 0.0],\n",
       " (20, 2, False): [0.0, 0.0],\n",
       " (20, 3, True): [0.0, 0.0],\n",
       " (20, 3, False): [0.0, 0.0],\n",
       " (20, 4, True): [0.0, 0.0],\n",
       " (20, 4, False): [0.0, 0.0],\n",
       " (20, 5, True): [0.0, 0.0],\n",
       " (20, 5, False): [0.0, 0.0],\n",
       " (20, 6, True): [0.0, 0.0],\n",
       " (20, 6, False): [0.0, 0.0],\n",
       " (20, 7, True): [0.0, 0.0],\n",
       " (20, 7, False): [0.0, 0.0],\n",
       " (20, 8, True): [0.0, 0.0],\n",
       " (20, 8, False): [0.0, 0.0],\n",
       " (20, 9, True): [0.0, 0.0],\n",
       " (20, 9, False): [0.0, 0.0],\n",
       " (20, 10, True): [0.0, 0.0],\n",
       " (20, 10, False): [0.0, 0.0],\n",
       " (21, 0, True): [0.0, 0.0],\n",
       " (21, 0, False): [0.0, 0.0],\n",
       " (21, 1, True): [0.0, 0.0],\n",
       " (21, 1, False): [0.0, 0.0],\n",
       " (21, 2, True): [0.0, 0.0],\n",
       " (21, 2, False): [0.0, 0.0],\n",
       " (21, 3, True): [0.0, 0.0],\n",
       " (21, 3, False): [0.0, 0.0],\n",
       " (21, 4, True): [0.0, 0.0],\n",
       " (21, 4, False): [0.0, 0.0],\n",
       " (21, 5, True): [0.0, 0.0],\n",
       " (21, 5, False): [0.0, 0.0],\n",
       " (21, 6, True): [0.0, 0.0],\n",
       " (21, 6, False): [0.0, 0.0],\n",
       " (21, 7, True): [0.0, 0.0],\n",
       " (21, 7, False): [0.0, 0.0],\n",
       " (21, 8, True): [0.0, 0.0],\n",
       " (21, 8, False): [0.0, 0.0],\n",
       " (21, 9, True): [0.0, 0.0],\n",
       " (21, 9, False): [0.0, 0.0],\n",
       " (21, 10, True): [0.0, 0.0],\n",
       " (21, 10, False): [0.0, 0.0],\n",
       " (22, 0, True): [0.0, 0.0],\n",
       " (22, 0, False): [0.0, 0.0],\n",
       " (22, 1, True): [0.0, 0.0],\n",
       " (22, 1, False): [0.0, 0.0],\n",
       " (22, 2, True): [0.0, 0.0],\n",
       " (22, 2, False): [0.0, 0.0],\n",
       " (22, 3, True): [0.0, 0.0],\n",
       " (22, 3, False): [0.0, 0.0],\n",
       " (22, 4, True): [0.0, 0.0],\n",
       " (22, 4, False): [0.0, 0.0],\n",
       " (22, 5, True): [0.0, 0.0],\n",
       " (22, 5, False): [0.0, 0.0],\n",
       " (22, 6, True): [0.0, 0.0],\n",
       " (22, 6, False): [0.0, 0.0],\n",
       " (22, 7, True): [0.0, 0.0],\n",
       " (22, 7, False): [0.0, 0.0],\n",
       " (22, 8, True): [0.0, 0.0],\n",
       " (22, 8, False): [0.0, 0.0],\n",
       " (22, 9, True): [0.0, 0.0],\n",
       " (22, 9, False): [0.0, 0.0],\n",
       " (22, 10, True): [0.0, 0.0],\n",
       " (22, 10, False): [0.0, 0.0],\n",
       " (23, 0, True): [0.0, 0.0],\n",
       " (23, 0, False): [0.0, 0.0],\n",
       " (23, 1, True): [0.0, 0.0],\n",
       " (23, 1, False): [0.0, 0.0],\n",
       " (23, 2, True): [0.0, 0.0],\n",
       " (23, 2, False): [0.0, 0.0],\n",
       " (23, 3, True): [0.0, 0.0],\n",
       " (23, 3, False): [0.0, 0.0],\n",
       " (23, 4, True): [0.0, 0.0],\n",
       " (23, 4, False): [0.0, 0.0],\n",
       " (23, 5, True): [0.0, 0.0],\n",
       " (23, 5, False): [0.0, 0.0],\n",
       " (23, 6, True): [0.0, 0.0],\n",
       " (23, 6, False): [0.0, 0.0],\n",
       " (23, 7, True): [0.0, 0.0],\n",
       " (23, 7, False): [0.0, 0.0],\n",
       " (23, 8, True): [0.0, 0.0],\n",
       " (23, 8, False): [0.0, 0.0],\n",
       " (23, 9, True): [0.0, 0.0],\n",
       " (23, 9, False): [0.0, 0.0],\n",
       " (23, 10, True): [0.0, 0.0],\n",
       " (23, 10, False): [0.0, 0.0],\n",
       " (24, 0, True): [0.0, 0.0],\n",
       " (24, 0, False): [0.0, 0.0],\n",
       " (24, 1, True): [0.0, 0.0],\n",
       " (24, 1, False): [0.0, 0.0],\n",
       " (24, 2, True): [0.0, 0.0],\n",
       " (24, 2, False): [0.0, 0.0],\n",
       " (24, 3, True): [0.0, 0.0],\n",
       " (24, 3, False): [0.0, 0.0],\n",
       " (24, 4, True): [0.0, 0.0],\n",
       " (24, 4, False): [0.0, 0.0],\n",
       " (24, 5, True): [0.0, 0.0],\n",
       " (24, 5, False): [0.0, 0.0],\n",
       " (24, 6, True): [0.0, 0.0],\n",
       " (24, 6, False): [0.0, 0.0],\n",
       " (24, 7, True): [0.0, 0.0],\n",
       " (24, 7, False): [0.0, 0.0],\n",
       " (24, 8, True): [0.0, 0.0],\n",
       " (24, 8, False): [0.0, 0.0],\n",
       " (24, 9, True): [0.0, 0.0],\n",
       " (24, 9, False): [0.0, 0.0],\n",
       " (24, 10, True): [0.0, 0.0],\n",
       " (24, 10, False): [0.0, 0.0],\n",
       " (25, 0, True): [0.0, 0.0],\n",
       " (25, 0, False): [0.0, 0.0],\n",
       " (25, 1, True): [0.0, 0.0],\n",
       " (25, 1, False): [0.0, 0.0],\n",
       " (25, 2, True): [0.0, 0.0],\n",
       " (25, 2, False): [0.0, 0.0],\n",
       " (25, 3, True): [0.0, 0.0],\n",
       " (25, 3, False): [0.0, 0.0],\n",
       " (25, 4, True): [0.0, 0.0],\n",
       " (25, 4, False): [0.0, 0.0],\n",
       " (25, 5, True): [0.0, 0.0],\n",
       " (25, 5, False): [0.0, 0.0],\n",
       " (25, 6, True): [0.0, 0.0],\n",
       " (25, 6, False): [0.0, 0.0],\n",
       " (25, 7, True): [0.0, 0.0],\n",
       " (25, 7, False): [0.0, 0.0],\n",
       " (25, 8, True): [0.0, 0.0],\n",
       " (25, 8, False): [0.0, 0.0],\n",
       " (25, 9, True): [0.0, 0.0],\n",
       " (25, 9, False): [0.0, 0.0],\n",
       " (25, 10, True): [0.0, 0.0],\n",
       " (25, 10, False): [0.0, 0.0],\n",
       " (26, 0, True): [0.0, 0.0],\n",
       " (26, 0, False): [0.0, 0.0],\n",
       " (26, 1, True): [0.0, 0.0],\n",
       " (26, 1, False): [0.0, 0.0],\n",
       " (26, 2, True): [0.0, 0.0],\n",
       " (26, 2, False): [0.0, 0.0],\n",
       " (26, 3, True): [0.0, 0.0],\n",
       " (26, 3, False): [0.0, 0.0],\n",
       " (26, 4, True): [0.0, 0.0],\n",
       " (26, 4, False): [0.0, 0.0],\n",
       " (26, 5, True): [0.0, 0.0],\n",
       " (26, 5, False): [0.0, 0.0],\n",
       " (26, 6, True): [0.0, 0.0],\n",
       " (26, 6, False): [0.0, 0.0],\n",
       " (26, 7, True): [0.0, 0.0],\n",
       " (26, 7, False): [0.0, 0.0],\n",
       " (26, 8, True): [0.0, 0.0],\n",
       " (26, 8, False): [0.0, 0.0],\n",
       " (26, 9, True): [0.0, 0.0],\n",
       " (26, 9, False): [0.0, 0.0],\n",
       " (26, 10, True): [0.0, 0.0],\n",
       " (26, 10, False): [0.0, 0.0],\n",
       " (27, 0, True): [0.0, 0.0],\n",
       " (27, 0, False): [0.0, 0.0],\n",
       " (27, 1, True): [0.0, 0.0],\n",
       " (27, 1, False): [0.0, 0.0],\n",
       " (27, 2, True): [0.0, 0.0],\n",
       " (27, 2, False): [0.0, 0.0],\n",
       " (27, 3, True): [0.0, 0.0],\n",
       " (27, 3, False): [0.0, 0.0],\n",
       " (27, 4, True): [0.0, 0.0],\n",
       " (27, 4, False): [0.0, 0.0],\n",
       " (27, 5, True): [0.0, 0.0],\n",
       " (27, 5, False): [0.0, 0.0],\n",
       " (27, 6, True): [0.0, 0.0],\n",
       " (27, 6, False): [0.0, 0.0],\n",
       " (27, 7, True): [0.0, 0.0],\n",
       " (27, 7, False): [0.0, 0.0],\n",
       " (27, 8, True): [0.0, 0.0],\n",
       " (27, 8, False): [0.0, 0.0],\n",
       " (27, 9, True): [0.0, 0.0],\n",
       " (27, 9, False): [0.0, 0.0],\n",
       " (27, 10, True): [0.0, 0.0],\n",
       " (27, 10, False): [0.0, 0.0],\n",
       " (28, 0, True): [0.0, 0.0],\n",
       " (28, 0, False): [0.0, 0.0],\n",
       " (28, 1, True): [0.0, 0.0],\n",
       " (28, 1, False): [0.0, 0.0],\n",
       " (28, 2, True): [0.0, 0.0],\n",
       " (28, 2, False): [0.0, 0.0],\n",
       " (28, 3, True): [0.0, 0.0],\n",
       " (28, 3, False): [0.0, 0.0],\n",
       " (28, 4, True): [0.0, 0.0],\n",
       " (28, 4, False): [0.0, 0.0],\n",
       " (28, 5, True): [0.0, 0.0],\n",
       " (28, 5, False): [0.0, 0.0],\n",
       " (28, 6, True): [0.0, 0.0],\n",
       " (28, 6, False): [0.0, 0.0],\n",
       " (28, 7, True): [0.0, 0.0],\n",
       " (28, 7, False): [0.0, 0.0],\n",
       " (28, 8, True): [0.0, 0.0],\n",
       " (28, 8, False): [0.0, 0.0],\n",
       " (28, 9, True): [0.0, 0.0],\n",
       " (28, 9, False): [0.0, 0.0],\n",
       " (28, 10, True): [0.0, 0.0],\n",
       " (28, 10, False): [0.0, 0.0],\n",
       " (29, 0, True): [0.0, 0.0],\n",
       " (29, 0, False): [0.0, 0.0],\n",
       " (29, 1, True): [0.0, 0.0],\n",
       " (29, 1, False): [0.0, 0.0],\n",
       " (29, 2, True): [0.0, 0.0],\n",
       " (29, 2, False): [0.0, 0.0],\n",
       " (29, 3, True): [0.0, 0.0],\n",
       " (29, 3, False): [0.0, 0.0],\n",
       " (29, 4, True): [0.0, 0.0],\n",
       " (29, 4, False): [0.0, 0.0],\n",
       " (29, 5, True): [0.0, 0.0],\n",
       " (29, 5, False): [0.0, 0.0],\n",
       " (29, 6, True): [0.0, 0.0],\n",
       " (29, 6, False): [0.0, 0.0],\n",
       " (29, 7, True): [0.0, 0.0],\n",
       " (29, 7, False): [0.0, 0.0],\n",
       " (29, 8, True): [0.0, 0.0],\n",
       " (29, 8, False): [0.0, 0.0],\n",
       " (29, 9, True): [0.0, 0.0],\n",
       " (29, 9, False): [0.0, 0.0],\n",
       " (29, 10, True): [0.0, 0.0],\n",
       " (29, 10, False): [0.0, 0.0],\n",
       " (30, 0, True): [0.0, 0.0],\n",
       " (30, 0, False): [0.0, 0.0],\n",
       " (30, 1, True): [0.0, 0.0],\n",
       " (30, 1, False): [0.0, 0.0],\n",
       " (30, 2, True): [0.0, 0.0],\n",
       " (30, 2, False): [0.0, 0.0],\n",
       " (30, 3, True): [0.0, 0.0],\n",
       " (30, 3, False): [0.0, 0.0],\n",
       " (30, 4, True): [0.0, 0.0],\n",
       " (30, 4, False): [0.0, 0.0],\n",
       " (30, 5, True): [0.0, 0.0],\n",
       " (30, 5, False): [0.0, 0.0],\n",
       " (30, 6, True): [0.0, 0.0],\n",
       " (30, 6, False): [0.0, 0.0],\n",
       " (30, 7, True): [0.0, 0.0],\n",
       " (30, 7, False): [0.0, 0.0],\n",
       " (30, 8, True): [0.0, 0.0],\n",
       " (30, 8, False): [0.0, 0.0],\n",
       " (30, 9, True): [0.0, 0.0],\n",
       " (30, 9, False): [0.0, 0.0],\n",
       " (30, 10, True): [0.0, 0.0],\n",
       " (30, 10, False): [0.0, 0.0],\n",
       " (31, 0, True): [0.0, 0.0],\n",
       " (31, 0, False): [0.0, 0.0],\n",
       " (31, 1, True): [0.0, 0.0],\n",
       " (31, 1, False): [0.0, 0.0],\n",
       " (31, 2, True): [0.0, 0.0],\n",
       " (31, 2, False): [0.0, 0.0],\n",
       " (31, 3, True): [0.0, 0.0],\n",
       " (31, 3, False): [0.0, 0.0],\n",
       " (31, 4, True): [0.0, 0.0],\n",
       " (31, 4, False): [0.0, 0.0],\n",
       " (31, 5, True): [0.0, 0.0],\n",
       " (31, 5, False): [0.0, 0.0],\n",
       " (31, 6, True): [0.0, 0.0],\n",
       " (31, 6, False): [0.0, 0.0],\n",
       " (31, 7, True): [0.0, 0.0],\n",
       " (31, 7, False): [0.0, 0.0],\n",
       " (31, 8, True): [0.0, 0.0],\n",
       " (31, 8, False): [0.0, 0.0],\n",
       " (31, 9, True): [0.0, 0.0],\n",
       " (31, 9, False): [0.0, 0.0],\n",
       " (31, 10, True): [0.0, 0.0],\n",
       " (31, 10, False): [0.0, 0.0]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the corresponding state-value function\n",
    "V_to_plot = dict((k,(k[0]>18)*(np.dot([0.8, 0.2],v)) + (k[0]<=18)*(np.dot([0.2, 0.8],v))) \\\n",
    "         for k, v in Q.items())\n",
    "\n",
    "# plot the state-value function\n",
    "plot_blackjack_values(V_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: MC Control\n",
    "\n",
    "In this section, you will write your own implementation of constant-$\\alpha$ MC control.  \n",
    "\n",
    "Your algorithm has four arguments:\n",
    "- `env`: This is an instance of an OpenAI Gym environment.\n",
    "- `num_episodes`: This is the number of episodes that are generated through agent-environment interaction.\n",
    "- `alpha`: This is the step-size parameter for the update step.\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "\n",
    "The algorithm returns as output:\n",
    "- `Q`: This is a dictionary (of one-dimensional arrays) where `Q[s][a]` is the estimated action value corresponding to state `s` and action `a`.\n",
    "- `policy`: This is a dictionary where `policy[s]` returns the action that the agent chooses after observing state `s`.\n",
    "\n",
    "(_Feel free to define additional functions to help you to organize your code._)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(observation_wrapper=observation_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control(agent, env, num_episodes, alpha, gamma=1.0):\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        episode = generate_episode_from_limit_stochastic(env)\n",
    "        agent.learn([episode], gamma)\n",
    "\n",
    "    return agent.policy_predictor, agent.quality_predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the cell below to obtain the estimated optimal policy and action-value function.  Note that you should fill in your own values for the `num_episodes` and `alpha` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the estimated optimal policy and action-value function\n",
    "policy_predictor, quality_predictor = mc_control(agent, env, 1000000, alpha=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we plot the corresponding state-value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = {}\n",
    "\n",
    "for current_sum in list(range(32)):\n",
    "    for dealers_card in list(range(11)):\n",
    "        for usable_ace in (True, False):\n",
    "            vals = []\n",
    "\n",
    "            for action in (0, 1):\n",
    "                obs = (current_sum, dealers_card, usable_ace)\n",
    "                vals.append(quality_predictor.predict_action_value(obs, action))\n",
    "\n",
    "            Q[obs] = vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# obtain the corresponding state-value function\n",
    "V = dict((k,np.max(v)) for k, v in Q.items())\n",
    "\n",
    "# plot the state-value function\n",
    "plot_blackjack_values(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we visualize the policy that is estimated to be optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = {}\n",
    "\n",
    "for current_sum in list(range(32)):\n",
    "    for dealers_card in list(range(11)):\n",
    "        for usable_ace in (True, False):\n",
    "            vals = []\n",
    "\n",
    "            for action in (0, 1):\n",
    "                obs = (current_sum, dealers_card, usable_ace)\n",
    "                vals.append(quality_predictor.predict_action_value(obs, action))\n",
    "\n",
    "            policy[obs] = 0 if vals[0] > vals[1] else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the policy\n",
    "plot_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **true** optimal policy $\\pi_*$ can be found in Figure 5.2 of the [textbook](http://go.udacity.com/rl-textbook) (and appears below).  Compare your final estimate to the optimal policy - how close are you able to get?  If you are not happy with the performance of your algorithm, take the time to tweak the decay rate of $\\epsilon$, change the value of $\\alpha$, and/or run the algorithm for more episodes to attain better results.\n",
    "\n",
    "![True Optimal Policy](images/optimal.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
